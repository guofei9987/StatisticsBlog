{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.math_ops.tanh>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.nn.relu\n",
    "tf.sigmoid\n",
    "tf.tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=tf.nn.relu(tf.matmul(x,w1)+biases1)\n",
    "y=tf.nn.relu(tf.matmul(a,w2)+biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob=tf.placeholder(tf.float32)#训练时小于1，预测时等于1，所以还是用placeholder\n",
    "hidden1_drop=tf.nn.dropout(hidden1,keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.nn.conv2d(x,kernel,strides=[1,1,1,1],padding='SAME')\n",
    "#strides[0,1]数表示卷积核的尺寸，\n",
    "#stride[2]表示channel数量，灰度图片是1，RGB图是3\n",
    "#stride[3]表示卷积核的数量\n",
    "#padding='SAME'表示边界加上padding，使得卷积的输入和输出保持同样的尺寸\n",
    "\n",
    "tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')#横竖步长为2，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.nn.lrn(pool1,4,bias=1,alpha=0.001/9.0,beta=0.75)\n",
    "#LRN模仿生物的侧抑制机制，对局部神经元创建竞争环境，使其中响应大的值相对更大，并抑制其它反馈小的神经元。从而增强泛化能力\n",
    "#可以用于Pooling之后，也可以用于conv之后、Pooling之前\n",
    "#适用于ReLu这种没有上界的激活函数，不适合Sigmoid这种有固定边界，或者能抑制过大值得激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数\n",
    "#### 1.交叉熵\n",
    "$H(p,q)=-\\sum p(x)\\log q(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy=-tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))\n",
    "# y_代表正确结果，y代表预测结果\n",
    "# tf.clip_by_value可以把张量中的数值限制在某个范围内,这里为了防止log0报错\n",
    "# reduce_mean:大概是求均值？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow对softmax+crossentropy进行了封装\n",
    "cross_entropy=tf.nn.softmax_cross_entropy_with_logits(y,y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.MSE\n",
    "$MSE(y,y')=\\dfrac{\\sum (y-y')^2}{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse=tf.reduce_mean(tf.square(y_-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.自定义\n",
    "例如，预测销量时，多预测一个损失1元，少预测1个损失10元。  \n",
    "$Loss(y,y')=\\sum f(y_i,y_i')$,\n",
    "$f(x,y)=\\left\\{\\begin{array}{ccc}a(x-y)&x>y\\\\\n",
    "b(y-x)&x\\leq y\\end{array}\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss=tf.reduce_sum(tf.select(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 学习率\n",
    "decayed_learning_rate=learning_rate * decay_rate ** (global_step/decay_steps)\n",
    "```\n",
    "tf.train.exponential_decay# 指数下降法减小学习率\n",
    "learning_rate=tf.train.exponential_decay(0.01,global_step=10000,decay_steps=100,decay_rate=0.96,staircase=True)\n",
    "```\n",
    "staircase=True时，(global_step/decay_steps)会转化成整数，导致学习率阶梯下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 iteration(s): x1 is 4.000000, learning rate is 0.096000.\n",
      "After 11 iteration(s): x11 is 0.690561, learning rate is 0.063824.\n",
      "After 21 iteration(s): x21 is 0.222583, learning rate is 0.042432.\n",
      "After 31 iteration(s): x31 is 0.106405, learning rate is 0.028210.\n",
      "After 41 iteration(s): x41 is 0.065548, learning rate is 0.018755.\n",
      "After 51 iteration(s): x51 is 0.047625, learning rate is 0.012469.\n",
      "After 61 iteration(s): x61 is 0.038558, learning rate is 0.008290.\n",
      "After 71 iteration(s): x71 is 0.033523, learning rate is 0.005511.\n",
      "After 81 iteration(s): x81 is 0.030553, learning rate is 0.003664.\n",
      "After 91 iteration(s): x91 is 0.028727, learning rate is 0.002436.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "TRAINING_STEPS = 100\n",
    "global_step = tf.Variable(0)\n",
    "LEARNING_RATE = tf.train.exponential_decay(0.1, global_step, 1, 0.96, staircase=True)\n",
    "\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y, global_step=global_step)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        if i % 10 == 0:\n",
    "            LEARNING_RATE_value = sess.run(LEARNING_RATE)\n",
    "            x_value = sess.run(x)\n",
    "            print (\"After %s iteration(s): x%s is %f, learning rate is %f.\"% (i+1, i+1, x_value, LEARNING_RATE_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过拟合\n",
    "cost Function加上L1 正则化或L2正则化  \n",
    "其中L1正则化可以将某些参数变成0，从而使参数系数。  \n",
    "L2不会使参数系数  \n",
    "(原因参见lasso和ridge regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.square(y_-y))+tf.contrib.layers.l2_regularizer(alpha)(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 滑动平均\n",
    "\n",
    "#### 1. 定义变量及滑动平均类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "v1 = tf.Variable(0, dtype=tf.float32)\n",
    "step = tf.Variable(0, trainable=False)\n",
    "ema = tf.train.ExponentialMovingAverage(0.99, step)\n",
    "maintain_averages_op = ema.apply([v1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 查看不同迭代中变量取值的变化。\n",
    "$\\min (decay,\\dfrac{1+num_{updates}}{10+num_{updates}})$\n",
    "一般情况下，decay设定为一个非常接近1的数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n",
      "[5.0, 4.5]\n",
      "[10.0, 4.5549998]\n",
      "[10.0, 4.6094499]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # 初始化\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print (sess.run([v1, ema.average(v1)]))\n",
    "    \n",
    "    # 更新变量v1的取值\n",
    "    sess.run(tf.assign(v1, 5))\n",
    "    sess.run(maintain_averages_op)#decay=0.1\n",
    "    print (sess.run([v1, ema.average(v1)])) #0.1*0+0.9*5=4.5\n",
    "    \n",
    "    # 更新step和v1的取值\n",
    "    sess.run(tf.assign(step, 10000))  \n",
    "    sess.run(tf.assign(v1, 10))\n",
    "    sess.run(maintain_averages_op)#decay=0.99\n",
    "    print (sess.run([v1, ema.average(v1)]) )      \n",
    "    \n",
    "    # 更新一次v1的滑动平均值\n",
    "    sess.run(maintain_averages_op)\n",
    "    print (sess.run([v1, ema.average(v1)])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加入collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_loss=tf.multiply(tf.nn.l2_loss(var),wl,name='weight_loss')#wl是L2_loss的系数\n",
    "tf.add_to_collection('losses',weight_loss)#加入一个collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正则化的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.add_n(tf.get_collection('losses'),name='total_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把entropy也加入'losses'使代码简洁一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.add_to_collection('losses',cross_entropy)\n",
    "tf.add_n(tf.get_collection('losses'),name='total_loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
